{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Hive Modeling\n",
    "# Part 2 - Optimization\n",
    "\n",
    "Sept 23 - lab 8  \n",
    "Sept 25 - lab 9\n",
    "\n",
    "<a href=\"#Creating-databases\">Create Database</a>  \n",
    "<a href=\"#Code-Syntax-(Create-and-Load)\">Create Database (Syntax)</a>  \n",
    "<a href=\"#Code-Syntax-(Working-with-database)\">Work with Database (Syntax)</a>  \n",
    "<a href=\"#Optimizing-Hive-Query-Performance\">Query Optimize</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Creating databases\n",
    "\n",
    "Managed database will be saved to `user/hive/warehouse/dualcore.db` and each table will have sub dir inside this directory  \n",
    "Tables = stored in multiple files `user/hive/warehouse/dualcore.db/table`\n",
    "\n",
    "External tables need to specify where we are storing the file\n",
    "\n",
    "\n",
    "**Data validation**  \n",
    "\"Schema on read\" meaning it wont check for errors when file format is incorrect  \n",
    "Null can be either true null or just our mistake\n",
    "\n",
    "\n",
    "**Table format (default)**  \n",
    "Plain text  \n",
    "One record per line by `\\n`  \n",
    "Column split by `A^`  \n",
    "$\\quad$ `B^` if members of arrays or structs  \n",
    "$\\quad$ `C^` if key value pairs\n",
    "\n",
    "**Row format (default)**  \n",
    "Delimited  \n",
    "Serde (text file formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optimizing Hive Query Performance\n",
    "\n",
    "### Use a faster execution engine (default is mapreduce)\n",
    "\n",
    "Tez - more flexible `set hive.execution.engine=tez;`  \n",
    "Spark - uses more memory but is faster than mapreduce `set hive.execution.engine=spark;`\n",
    "\n",
    "\n",
    "\n",
    "### Use faster storage formats (default is textfile)\n",
    "\n",
    "Depends on...  \n",
    "Ingest pattern - where we get our data from  \n",
    "Tool compatibility  \n",
    "Expected lifetime  \n",
    "Storage and performance reqs\n",
    "\n",
    "<img src=https://i.imgur.com/KQ31PM9.png width=\"400\" height=\"340\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binary encoding**  \n",
    "Stores data as key-value pairs\n",
    "\n",
    "**Columnar**   \n",
    "Data stored down the column, not across the rows (\"stored by column\")  \n",
    "Pro: efficient when we only need a subset of table (doesn't have to store duplicates)\n",
    "\n",
    "**Textfile (Default)**  \n",
    "Pro: Read or written from pretty much and programming language  \n",
    "Con: Not great at scale - numbers are stored as strings, hard to represent binary\n",
    "\n",
    "**1. SequenceFiles (binary encoding)** store key-value pairs  \n",
    "Pro: Much more efficient  \n",
    "Con: Doesn't work well with other tools, Java specific\n",
    "\n",
    "**2. Apache Avro (binary encoding)**  \n",
    "Pro: efficient, supported thruout Hadoop, good for long term storage  \n",
    "\n",
    "**3. Apache Parquet (columnar and binary encoding)**  \n",
    "Pro: reduced storage performance and improved performance, best when adding many records at once\n",
    "\n",
    "**4. RCFile (columnar and binary encoding)** stores different row groups into a columnar table  \n",
    "Con: pretty poor performance\n",
    "\n",
    "**5. ORCFile (columnar and binary encoding)** stores different row groups into a columnar table  \n",
    "Pro: better performance than RCFile, works well with Hive and Spark\n",
    "\n",
    "`CREATE TABLE tab2 (\n",
    "    order_id INT,\n",
    "    prod_id INT)\n",
    "    STORED AS PARQUET` or whatever format we want\n",
    "    \n",
    "`INSERT OVERWRITE TABLE tab1\n",
    "    SELECT * FROM TAB2;`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Optimizing Hive Query Performance (pt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Partitions - only read part of the data\n",
    "\n",
    "Subdivides data based on values  \n",
    "Partioned field is removed from table\n",
    "\n",
    "**Use when**  \n",
    "Reading takes too long  \n",
    "Queries typically filter on partitioned columns  \n",
    "Not too many different values per part col  \n",
    "When data comes in by month or something like that  \n",
    "Part cols need to be in the data\n",
    "\n",
    "\n",
    "**Example**  \n",
    "Web log data, 100gb per day. Want to run ML on the data.  \n",
    "Should partition table on the day  \n",
    "Would be better with static with a macro to run the partition every day  \n",
    "(Note that dynamic is slower because Hive needs to look into the file)  \n",
    "(If we were partioning by state, dynamic would be better)\n",
    "\n",
    "\n",
    "**Example**  \n",
    "Hive dataset (5tb). Want to take samples and run ML on.  \n",
    "Approach to obtain sample - bucketing (gives randomness based on hash)  \n",
    "Optimization technique - use sort by with a random generator for id\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Loading data into partitioned tables\n",
    "\n",
    "To enable dynamic  \n",
    "hive.exec.dynamic.partition true  \n",
    "hive.exec.dynamic.partition.mode nonstrict\n",
    "\n",
    "#### 1 - Dynamic - automatic done based on col values  \n",
    "\n",
    "**When creating table**  \n",
    "`create external table __ (\n",
    "partitioned by (state STRING)\n",
    "...  `\n",
    "\n",
    "\n",
    "**When altering table**  \n",
    "`insert overwrite table __\n",
    "partition column\n",
    "select col, col, partition_col \n",
    "from __`\n",
    "\n",
    "\n",
    "#### 2 - Static - manually creating partitions  \n",
    "\n",
    "**Adding**  \n",
    "`alter table __\n",
    "add partition (col=val)`\n",
    "\n",
    "**Dropping**  \n",
    "`alter table __\n",
    "drop partition (col=val)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### View partitions\n",
    "\n",
    "`show partitions table;`\n",
    "\n",
    "---\n",
    "\n",
    "### Better code\n",
    "\n",
    "Filter rows early, only show relevant cols  \n",
    "Use `explain` before query to show query plan \n",
    "\n",
    "`order by` uses single reducer (global sort)  \n",
    "`sort by` sorts input before going to reducer **preferred**  \n",
    "\n",
    "---\n",
    "\n",
    "### Bucketing - subdivide data\n",
    "\n",
    "Best when we want samples  \n",
    "Data into separate **files**  \n",
    "Partition was into separate fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 8 (Sept 23) - How do we create Hive Tables?\n",
    "\n",
    "\n",
    "\n",
    "https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab08-datamodel.html  \n",
    "https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab08-datamodel-solution.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1 - Create table\n",
    "\n",
    "First way - using Sqoop because data already in mySQL  \n",
    "We are creating the data + metadata here  \n",
    "Because we created, this is managed table\n",
    "\n",
    "`sqoop import \\`  \n",
    "` --connect jdbc:mysql://localhost/dualcore \\`  \n",
    "` --username root --password cloudera \\`  \n",
    "` --fields-terminated-by '\\t' \\`  \n",
    "` --hive-table dualcore.suppliers \\`  download to this database  \n",
    "` --hive-import`                      special command (also creates metadata, not just data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "`hadoop fs -cat /dualcore/employees/part-m-00004 | head` to see how cols are separated\n",
    "\n",
    "### Step 2 - External Table\n",
    "\n",
    "(within hue)  \n",
    "Create an external tables based on **employees**  \n",
    "`DROP TABLE IF EXISTS employees;`  \n",
    "`CREATE EXTERNAL TABLE dualcore.employees(\n",
    "emp_id\tSTRING\n",
    "fname\tSTRING\n",
    "lname\tSTRING\n",
    "address\tSTRING\n",
    "city\tSTRING\n",
    "state\tSTRING\n",
    "zipcode\tSTRING\n",
    "job_title\tSTRING\n",
    "email\tSTRING\n",
    "active\tSTRING\n",
    "salary\tINT)`  \n",
    "`row format delimited\n",
    "    fields terminated by '\\t'`  need to do this to specify a different col separator  \n",
    "`location '/dualcore/employees'`;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3 - Create Internal table\n",
    "\n",
    "\n",
    "`CREATE table default.ratings (\n",
    "posted\tTIMESTAMP,\n",
    "cust_id\tINT,\n",
    "prod_id\tINT,\n",
    "rating\tTINYINT,\n",
    "message\tSTRING)\n",
    "ROW FORMAT delimited\n",
    "FIELDS TERMINATED BY '\\t';`\n",
    "\n",
    "`describe formatted ratings;` to see info and where table is located (managed vs external)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3b - Upload data to internal\n",
    "\n",
    "**With hadoop**  \n",
    "`hadoop fs -put` $`ADIR/data/ratings_2012.txt \\\n",
    " /user/hive/warehouse/ratings`\n",
    "\n",
    "\n",
    "**With load data inpath**  \n",
    "Put the data in dualcore  \n",
    "`hadoop fs -put` $`ADIR/data/ratings_2013.txt /dualcore`\n",
    "\n",
    "Verify its there  \n",
    "`hadoop fs -ls /dualcore/ratings_2013.txt`\n",
    "\n",
    "(within hue)  \n",
    "`LOAD DATA INPATH '/dualcore/ratings_2013.txt' INTO TABLE ratings;`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4 - Creating table with complex fields\n",
    "\n",
    "` CREATE TABLE loyalty_program\n",
    " (cust_id INT,\n",
    "     fname STRING,\n",
    "     lname STRING,\n",
    "     email STRING,\n",
    "     level STRING,\n",
    "     phone MAP<STRING, STRING>,\n",
    "     order_ids ARRAY<INT>,\n",
    "     order_value STRUCT< min:INT, max:INT, avg:INT, total:INT>)\n",
    " ROW FORMAT DELIMITED\n",
    "     FIELDS TERMINATED BY '|'`  \n",
    "     `COLLECTION ITEMS TERMINATED BY ','`  for array and struct  \n",
    "     `MAP KEYS TERMINATED BY ':';` for map\n",
    "     \n",
    "     ---\n",
    "     \n",
    "This is a `local` file, not in `HDFS`  \n",
    "`LOAD DATA LOCAL INPATH '/home/cloudera/training_materials/analyst/exercises/data_mgmt/loyalty_data.txt' \n",
    "INTO TABLE loyalty_program;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Map**  \n",
    "`from loyalty_program select phone['HOME']\n",
    "where cust_id = 1200866;`\n",
    "\n",
    "**Array**  \n",
    "`from loyalty_program select phone[2]\n",
    "where cust_id = 1200866;`\n",
    "\n",
    "**Struct**  \n",
    "`from loyalty_program select ordervalue.total\n",
    "where cust_id = 1200866;`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lab 8x - csv from the internet\n",
    "\n",
    "[Lab](https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab09-hiveopt.html)    \n",
    "[Solution](https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab09-hiveopt-solution.html)\n",
    "\n",
    "**Download file**  \n",
    "`wget http://idsdl.csom.umn.edu/c/share/MSBA6330/titanic.csv`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create the managed table**  \n",
    "`CREATE TABLE titanic\n",
    "(passengerid int,survived int,pclass int,name string,\n",
    "sex string, age int,sibsp int,parch int,ticket string,\n",
    "fare string,cabin string,embarked string)`\n",
    "\n",
    "`ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\n",
    "WITH SERDEPROPERTIES (`  \n",
    "`\"separatorChar\"=\",\",`  -- default: comma separated  \n",
    "`\"quoteChar\"=\"\\\"\",` -- default: use double quotes as quoteChar  \n",
    "`\"escapeChar\"=\"\\\\\"`  -- using \\ as escapeChar.  \n",
    ")  \n",
    "`tblproperties (\"skip.header.line.count\"=\"1\");` -- skip 1 header line\n",
    "\n",
    "\n",
    "**Load the data in**  \n",
    "`pwd` to see where the data was downloaded  \n",
    "`load data local inpath \"/home/cloudera/titanic.csv\" into table titanic;`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Lab 9 (Sept 25) - Hive Optimization\n",
    "\n",
    "[Lab](https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab09-hiveopt.html)  \n",
    "[Solution](https://pages.github.umn.edu/deliu/bigdata19/04-Hive2/lab09-hiveopt-solution.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create directory  \n",
    "`cd ADIR/exercises/\n",
    "mkdir hive_optimization\n",
    "cd hive_optimization`\n",
    "\n",
    "download data  \n",
    "`wget http://idsdl.csom.umn.edu/c/share/msba6330/movie.zip`\n",
    "\n",
    "Create a Hadoop directory /user/cloudera/movies:  \n",
    "`hadoop fs -mkdir /user/cloudera/movies`\n",
    "\n",
    "unzip data and put into HDFS - avoids downloading to local first  \n",
    "`unzip -p movie.zip | hadoop fs -put - /user/cloudera/movies/movie.txt`  \n",
    "Note that `p` gets rids of messages  \n",
    "\n",
    "how many lines?  \n",
    "`hadoop fs -ls movies`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 1 - Standalone\n",
    "\n",
    "**Using hue**  \n",
    "Run query - top 10 brands by sales  \n",
    "`SELECT brand, COUNT(prod_id) AS num\n",
    "FROM products\n",
    "GROUP BY brand\n",
    "ORDER BY num DESC\n",
    "LIMIT 10;`\n",
    "\n",
    "prefix the above with `explain` to see steps\n",
    "\n",
    "Enable local mode  \n",
    "`SET mapreduce.framework.name=local;`\n",
    "\n",
    "Go back to mapreduce mode  \n",
    "`SET mapreduce.framework.name=yarn;` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 3 - ORC\n",
    "\n",
    "Create database & table  \n",
    " `create database movies;\n",
    " CREATE EXTERNAL TABLE movies.movie_raw (\n",
    "     id INT,\n",
    "     name STRING,\n",
    "     year int\n",
    "     ) ROW FORMAT DELIMITED  FIELDS TERMINATED BY '|' LOCATION '/user/cloudera/movies/';`\n",
    "\n",
    "\n",
    "Now create a hive-managed ORC table movie_orc.  \n",
    " `CREATE TABLE movies.movie_orc (\n",
    "     id INT,\n",
    "     name STRING,\n",
    "     year int\n",
    "     ) STORED AS ORC;`\n",
    "\n",
    "\n",
    "Load data into the orc table.  \n",
    "` INSERT INTO TABLE movie_orc \n",
    " SELECT *\n",
    " FROM movie_raw;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Compare Disk size\n",
    "\n",
    "`-du` = calc disk size  \n",
    "`-h` = argument for du\n",
    "\n",
    "**External table**  \n",
    "`hadoop fs -du -h /user/cloudera/movies`  \n",
    " 43.6 M  43.6 M  /user/cloudera/movies/movie.txt\n",
    "\n",
    "**Managed table**  \n",
    "` hadoop fs -du -h /user/hive/warehouse/movies.db`  \n",
    " 15.9 M  15.9 M  /user/hive/warehouse/movies.db/movie_orc\n",
    " \n",
    "Run a query  \n",
    "`select year, count(*) from movie_orc group by year`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Step 4 - Using partitioned tables\n",
    "\n",
    "Create partitioned table (3 fields + 1 virtual field)  \n",
    " `CREATE TABLE movies.movie_orc_partitioned (\n",
    "     id INT,\n",
    "     name STRING,\n",
    "     year INT\n",
    " ) partitioned by (decade int) \n",
    " STORED AS ORC;`\n",
    "\n",
    "We are going to use **dynamic** because we dont currently have a decade column  \n",
    "Enable nonstrict dynamic partitioning  \n",
    " `SET hive.exec.dynamic.partition = True;`  \n",
    " `SET hive.exec.dynamic.partition.mode = nonstrict;`\n",
    "\n",
    "Loading data into movie_orc_partitioned.  \n",
    "` INSERT INTO TABLE movies.movie_orc_partitioned PARTITION(decade) \n",
    " SELECT id,name,year, floor(year/10)*10\n",
    " FROM movies.movie_orc;`\n",
    "\n",
    "Compare the performance of the following two queries.  \n",
    "` select count(*) from movie_orc where floor(year/10)*10=1950;\n",
    " select count(*) from movie_orc_partitioned where decade=1950;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Creating buckets from movie_orc\n",
    "\n",
    "`create table movie_bucketed\n",
    "(id INT,\n",
    "name STRING,\n",
    "year INT\n",
    ") CLUSTERED BY (id)\n",
    "into 10 buckets;`\n",
    "\n",
    "`import overwrite table movie_bucketed\n",
    "select * from movie_orc;`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a query on one of the buckets  \n",
    "2nd bucket out of the first set of 5  \n",
    "`select count(*) from movie_bucketed\n",
    "tablesample (bucket 2 out of 5 on id);`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
