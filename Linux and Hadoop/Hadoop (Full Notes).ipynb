{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hadoop Overview\n",
    "\n",
    "<a href=\"#MapReduce\">MapReduce for Processing</a>  \n",
    "<a href=\"#HDFS\">HDFS for Storing</a>  \n",
    "<a href=\"#Sqoop\">Sqoop for taking in data</a>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Why Hadoop?\n",
    "\n",
    "Its very cheap to store large amounts of data, but its hard to process with traditional tools  \n",
    "Prob 1 = how can we **reliably** store large data?  \n",
    "Prob 2 = how can we analyze all this data? Takes lots of **time** to read data into memory  \n",
    "\n",
    "Hadoop is the solution to this - scalable, reliable, available, fast, economical  \n",
    "1) HDFS = Reliable, distributes data across a cluster   \n",
    "2) MapReduce = framework for parallel processing  \n",
    "\n",
    "Hadoop is run on a collection of servers - as opposed to running on a \"super computer\"  \n",
    "ie Hadoop used horizontal scaling (adding more machines)  \n",
    "Node = name of each server  \n",
    "Each node stores & processes data (called data locality)  \n",
    "\n",
    "Hadoop 2.0 supports non MapReduce applications, also supports interactive and streaming applications (that will not use MapReduce)\n",
    "\n",
    "#### Benefits of Hadoop\n",
    "\n",
    "1. Fault tolerance    \n",
    "Hadoop has built in data redundancy (3 copies) in case one of the nodes fails  \n",
    "\n",
    "---\n",
    "\n",
    "#### Hadoop Trends\n",
    "\n",
    "(Old) `Data warehouse` --> need to clean and structure before storing  \n",
    "(New) `Data Lakes` --> able to store structured and unstructured raw data  \n",
    "$\\quad$ Hadoop is a platform for building a data lake  \n",
    "$\\quad$ Issue: Can we actually use all this to add value?  \n",
    "$\\quad$ Issue: Hard to work with, especially with multiple people\n",
    "\n",
    "**Cloud computing**  \n",
    "On premise big data providers - Cloudera  \n",
    "Cloud computing providers - AWS, Azure, Google Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Hadoop Ecosystem - All of these are scalable and distributed\n",
    "\n",
    "<img src=https://i.imgur.com/NktUrwM.png width=\"500\" height=\"440\" align=\"left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TASK 1 - Processing Layer (eg MapReduce)\n",
    "\n",
    "##### Batch Processing Tools (Top left)\n",
    "\n",
    "1. Hadoop MapReduce\n",
    "\n",
    "2. Apache Pig - offers higher level data processing than Hadoop  \n",
    "Useful for ETL - commands are more \"human friendly\", similar to SQL language\n",
    "\n",
    "3. Apache Hive - data warehouse application for Hadoop  \n",
    "Similar to SQL but for big data\n",
    "\n",
    "4. Spark - fast in-memory processing engine\n",
    "\n",
    "##### Interactive Query Tools\n",
    "\n",
    "1. Apache Impala - interactive SQL engine, low latency (interactive query tool)  \n",
    "\n",
    "2. Apache Drill - high performance SQL engine  \n",
    "Can query semi structured data (eg HDFS, HBase, JSON, MongoDB, Amazon S3)\n",
    "\n",
    "##### Machine Learning\n",
    "\n",
    "1. Apache Mahout - ML for Hadoop\n",
    "\n",
    "2. Apache Spark MLlib - ML component of Spark\n",
    "\n",
    "3. H2O - in-memory, distributed - can be used with R, Python\n",
    "\n",
    "\n",
    "##### Streaming\n",
    "\n",
    "1. Apache Storm\n",
    "\n",
    "2. Spark Streaming\n",
    "\n",
    "3. Apache Flink"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# MapReduce\n",
    "`processing`\n",
    "\n",
    "\n",
    "<img src=https://i.imgur.com/JcxE2Sa.jpg width=\"400\" height=\"340\" align=\"left\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "MapReduce is a programming model, typically uses Java (also supports Python)  \n",
    "Follows the \"shared-nothing\" architecture - tasks are not dependent on each other\n",
    "\n",
    "**2 functions, both written by the user**  \n",
    "Map - takes input pair, produces set of intermediate pairs, then groups values that have the same key and pass to reduce  \n",
    "$\\quad$ The value is a row from the input  \n",
    "Reduce - accepts intermediate key + values, shuffle and sort, merges values together  \n",
    "$\\quad$ The values a collection of all values for each key\n",
    "\n",
    "**Example**  \n",
    "`Input` - Take input of url and page  \n",
    "`Map output` Key = url, Value = series of key value pairs. Ultimately value is a  pair of key=term, value=frequency  \n",
    "`Reduce output` Key = url, Value = term freq pairs compressed into one\n",
    "\n",
    "\n",
    "#### Benefits of MapReduce\n",
    "\n",
    "1. Simple compared to other distributed programming models  \n",
    "2. Flexible - handles data like images, video, etc  \n",
    "3. Scalable - able to finish sooner by adding more workers\n",
    "\n",
    "Quickly losing ground to Spark and other engines\n",
    "\n",
    "\n",
    "**debugging before running thru mapreduce**  \n",
    "`hadoop fs -cat /dualcore/employees/* \\\n",
    "| head -n 100 \\\n",
    "| python mapper.py \\\n",
    "| sort \\\n",
    "| python reducer.py`\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Lab 2 - MapReduce\n",
    "\n",
    "https://pages.github.umn.edu/deliu/bigdata19/02-Hadoop/lab02-mapreduce.html  \n",
    "https://pages.github.umn.edu/deliu/bigdata19/02-Hadoop/lab02-mapreduce-solution.html\n",
    "\n",
    "\n",
    "for each state, how many employees earn > 75k?\n",
    "\n",
    "**Step 1 - enter the directory**  \n",
    "`cd ADIR/exercises/data_ingest/bonus_01`  \n",
    "`ls -l` to see the files\n",
    "\n",
    "**Step 2 - clean output destination**  \n",
    "`rm results.txt\n",
    "hadoop fs -rm -r /user/cloudera/empcounts`\n",
    "\n",
    "\n",
    "**Step 3 - view mapper, reducer, and runjob shell script**  \n",
    "`cat mapper.py\n",
    "cat reducer.py\n",
    "cat runjob.sh`\n",
    "\n",
    "\n",
    "**Step 4 - verify results**  \n",
    "`hadoop fs -ls /user/cloudera/empcounts` does this directory already exist?  \n",
    "`hadoop fs -cat input_data | head -n 100 | python mapper.py | sort | python reducer.py` to debug  \n",
    "`./runjob.sh` **to run the job**  \n",
    "`hadoop fs -getmerge /user/cloudera/empcounts results.txt` to download the results to a local file  \n",
    "`less results.txt` to view results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# TASK 2 - Storage Layer (eg HDFS)\n",
    "\n",
    "1. HDFS (Hadoop Distributed File System)  \n",
    "Good - inexpensive, scalable, reliable\n",
    "Issue - high latency (long delays)\n",
    "\n",
    "2. Apache HBase (key-store storage)  \n",
    "noSQL database built on HDFS  \n",
    "Scales very well, high thruput\n",
    "Maps row key, col key, timestamp to find data  \n",
    "Issue - no high-level query language, no SQL support, API access only  \n",
    "Not really good for analytics\n",
    "\n",
    "3. Apache Kudu  \n",
    "Designed for analytics - columnar storage  \n",
    "Good for SQL  \n",
    "Not built on HDFS\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# HDFS \n",
    "`storing`\n",
    "\n",
    "Files stored in HDFS are not visisble to host's file system. Have to use a special utility (e.g., hadoop fs or a web-based utility) to view files in HDFS.\n",
    "\n",
    "Optimized for millions of large files (likely over 100 mb)  \n",
    "Hierarchical directory storage  \n",
    "\n",
    "**Differences between Hadoop and Linux**  \n",
    "No current directory  \n",
    "CANNOT modify files once they are written  \n",
    "Have to use a special utility to access files\n",
    "\n",
    "---\n",
    "\n",
    "#### HDFS Structure\n",
    "\n",
    "NameNode contains the files  \n",
    "Files break down into blocks  \n",
    "Blocks are duplictaed 3 times and go into the DataNodes  \n",
    "A Java Virtual Machine is required to process a block  \n",
    "$\\quad$ if we have too many blocks, scalability is limited\n",
    "\n",
    "**More detailed look of ^**  \n",
    "Files are broken into blocks and then duplicated 3 times  \n",
    "$\\quad$ Typical HDFS block = 128 mb (128000 kb)  \n",
    "$\\quad$ Typical windows block = 4 kb    \n",
    "\n",
    "Each cluster runs as master/slave  \n",
    "Master = NameNode (1 or 2) - keeps track of files, blocks, DataNodes  \n",
    "$\\quad$ NameNodes store this info in memory (rule of thumb - each block takes 150 bytes of memory)  \n",
    "Slave = DataNodes (many) - read and write the actual data\n",
    "\n",
    "#### Copying Info \n",
    "\n",
    "hadoop fs -put == copy local files to HDFS  \n",
    "hadoop fs -get == copy HDFS to local files\n",
    "\n",
    "\n",
    "### LAB\n",
    "\n",
    "https://pages.github.umn.edu/deliu/bigdata19/02-Hadoop/lab01-hdfs.html  \n",
    "https://pages.github.umn.edu/deliu/bigdata19/02-Hadoop/lab01-hdfs-solution.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# TASK 3 - Integration Layer\n",
    "\n",
    "1. HDFS - direct file transfer\n",
    "\n",
    "2. Apache Sqoop - mainly for moving relational data between Hadoop and other databases\n",
    "\n",
    "3. Apache Kafka - for streaming data, messaging systems\n",
    "\n",
    "4. Apache Flume - for streaming data, messaging systems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sqoop\n",
    "`Transfer data between RDMS and Hadoop`\n",
    "\n",
    "Sqoop is a command line utility mainly used for ingesting data from RDBSMS, then importing as comma delimited text file (default)\n",
    "\n",
    "Performs tasks using mapreduce jobs (typically 4 parallel when importing, splits the jobs up by primary key).  \n",
    "When importing, creates a new directory in HDFS.  \n",
    "\n",
    "We can import partial tables by specifying which `--column`s we want, or by adding a filter that the rows will need to meet with `--where`.  \n",
    "Incremental imports with `--incremental`. Will need to include a `--check-column` and `--last-value` to specify where to increment\n",
    "\n",
    "**Techniques**  \n",
    "`import`  \n",
    "`import-all-tables`  \n",
    "`export`  \n",
    "`list-tables`\n",
    "\n",
    "---\n",
    "\n",
    "**Example - import order_details**  \n",
    "**Step 1 - see our main directories in the local host**  \n",
    "`hadoop fs -ls /` \n",
    "\n",
    "**Step 2 - Import tables into dualcore folder**  \n",
    "`sqoop import \\  \n",
    "--connect jdbc:mysql://localhost/dualcore \\  \n",
    "--username training --password training \\  \n",
    "--fields-terminated-by '\\t' \\  \n",
    "--warehouse-dir /dualcore \\  \n",
    "--table order_details \\  \n",
    "--split-by=order_id`  # if there isnt one single primary key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
