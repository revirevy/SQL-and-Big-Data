{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark SQL\n",
    "\n",
    "HW 5, Labs 5 and 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accountdevice.tsv  calllog.tsv\t\t   device.tsv\t      webpage.tsv\r\n",
      "accounts.tsv\t   customerservicerep.tsv  knowledgebase.tsv  websitehit.json\r\n",
      "basestations.tsv   device.json\t\t   webpage.json       websitehit.tsv\r\n"
     ]
    }
   ],
   "source": [
    "# Get the files\n",
    "! wget http://idsdl.csom.umn.edu/c/share/sparkdata.zip\n",
    "! unzip sparkdata.zip\n",
    "\n",
    "# Files in the directory\n",
    "!ls loudacre\n",
    "\n",
    "# First lines of relevant files\n",
    "!head -2 loudacre/webpage.json    # devices\n",
    "!head -2 loudacre/device.json     # webpages\n",
    "!head -2 loudacre/websitehit.json # hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the file directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infer schema\n",
    "bids = spark.read.option(\"inferSchema\",\"true\").csv(data_file)\n",
    "bids = bids.toDF(*cols)\n",
    "\n",
    "df = spark.read.json(\"file:/databricks/driver/yelp.json\")\n",
    "df.printSchema()\n",
    "df.take(5)\n",
    "\n",
    "\n",
    "data = spark.read.option(\"header\", \"true\") \\\n",
    ".option(\"delimiter\", \"\\\\t\") \\\n",
    ".csv(\"/databricks-...\") \\\n",
    ".cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Col names to append\n",
    "schema_str = \"\"\"\n",
    "    auctionid long, bid double, bidtime double, bidder string,\n",
    "    bidrate long, openbid double, price double,  \n",
    "    itemtype string, dtl long\n",
    "\"\"\"\n",
    "\n",
    "# Re-read the file\n",
    "bids = spark.read.schema(schema_str).format(\"csv\").load(\"sparkdata/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#StructType\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#define a structtype as schema\n",
    "schema_structtype = StructType([\n",
    " StructField(\"auctionid\",LongType(),True),\n",
    " StructField(\"bid\",DoubleType(),True),\n",
    " StructField(\"bidtime\",DoubleType(),True),\n",
    " StructField(\"bidder\",StringType(),True),\n",
    " StructField(\"bidrate\",LongType(),True),\n",
    " StructField(\"openbid\",DoubleType(),True),\n",
    " StructField(\"price\",DoubleType(),True),\n",
    " StructField(\"itemtype\",StringType(),True),\n",
    " StructField(\"dtl\",LongType(),True)\n",
    "])\n",
    " \n",
    "bids = spark.read.schema(schema_structtype).csv(\"sparkdata/auctiondata.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_file = \"sparkdata/auctiondata.csv\"\n",
    "rawDataRDD = sc.textFile(data_file).cache()\n",
    "\n",
    "\n",
    "from pyspark.sql import Row\n",
    "csvRDD = rawDataRDD.map(lambda l: l.split(\",\"))\n",
    "\n",
    "# convert\n",
    "rowRDD = csvRDD.map(lambda p: Row(\n",
    "    auctionid=p[0], \n",
    "    bid=float(p[1]),\n",
    "    bidtime=float(p[2]),\n",
    "    bidder=p[3],\n",
    "    bidrate=int(p[4]),\n",
    "    openbid=float(p[5]),\n",
    "    price=float(p[6]),\n",
    "    itemtype=p[7],\n",
    "    dtl=int(p[8]),\n",
    "    )\n",
    ")\n",
    "\n",
    "bids = spark.createDataFrame(rowRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Working with dataframe\n",
    "\n",
    "#### Filter, select, sort, show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bids.filter(bids.auctionid==1645914432) \\\n",
    "    .select(\"bid\",\"bidder\",\"bidtime\").sort(bids.bidtime.desc()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View with sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bids.createOrReplaceTempView(\"bids\")\n",
    "itemtypes = spark.sql(\"\"\"\n",
    "    SELECT itemtype, max(price) as max_price, count(*) as num_bids \n",
    "    from bids \n",
    "    group by itemtype\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas agg function instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "bids.groupBy(\"itemtype\").agg(f.max(bids.price).alias(\"max_price\"),f.count(bids.price).alias(\"num_bids\")).show()\n",
    "\n",
    "maxprices = bids.select(\"auctionid\",\"itemtype\",\"price\") \\\n",
    "    .filter(bids.itemtype=='cartier') \\\n",
    "    .groupBy(\"auctionid\") \\\n",
    "    .max(\"price\") \\\n",
    "    .withColumnRenamed(\"max(price)\",\"max_price\")\n",
    "\n",
    "\n",
    "# Unique values\n",
    "df.select('column').distinct.take()\n",
    "\n",
    "# Getting ready to value_counts\n",
    "df.select('col_id', \n",
    "          (f.explode(df.column)).alias('alias_name'))\n",
    "\n",
    "# Group by and count\n",
    "df.groupBy(df.col)\n",
    "    .agg(f.count(df.id)\n",
    "        .alias('alias'))\n",
    "    .sort('other_col', ascending=False)\n",
    "    .take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write dataframe into file\n",
    "maxprices.write.csv(\"maxprices\")\n",
    "\n",
    "# Verify - there are multiple files, parallel processing (each partition of your data may write its own output)\n",
    "!ls -l maxprices/\n",
    "\n",
    "# Take all data into one file\n",
    "! cat maxprices/* > maxprices.csv\n",
    "\n",
    "# Head of the file\n",
    "! head maxprices.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage = spark.read.json(\"loudacre/webpage.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does it look? (using .show, converting to pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webpage.printSchema()\n",
    "\n",
    "# Print 3 lines\n",
    "webpage.show(3, truncate=False)\n",
    "\n",
    "# Convert to local, then to pandas, print 3 lines\n",
    "webpage.limit(3).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First field has a list of values. Create a new df to explode them so that each element gets its own line + web_page_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n",
    "page_files = webpage.select(\"web_page_num\", \\\n",
    "             f.explode(f.split(webpage.associated_files,\",\")).alias(\"assoc_file\"))\n",
    "\n",
    "# Verify\n",
    "page_files.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inner join this new df with the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "webpage_files = webpage.join(page_files, \"web_page_num\") \\\n",
    "    .select(\"web_page_num\",\"web_page_file_name\",page_files.assoc_file)\n",
    "    \n",
    "# Verify\n",
    "webpage_files.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### for each webpage, what are the top 2 devices used for visiting this page? \n",
    "\n",
    "window functions with `rank()`  \n",
    "[DataBricks WindowFunc](https://databricks.com/blog/2015/07/15/introducing-window-functions-in-spark-sql.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hits = spark.read.json(\"sparkdata/loudacre/websitehit.json\")\n",
    "devices = spark.read.json(\"sparkdata/loudacre/device.json\")\n",
    "hits.show(3)\n",
    "devices.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View - Hits after grouping by page and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Replacing the file with a view so we can use sql commands, operates in place\n",
    "hits.createOrReplaceTempView(\"hits\")\n",
    "\n",
    "stat = spark.sql(\"\"\"\n",
    "    select web_page_id, device_id, count(*) as hits \n",
    "    from hits \n",
    "    group by web_page_id, device_id \n",
    "    order by web_page_id, count(*) desc\n",
    "\"\"\").cache().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Window function - create partitions by web page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "wind = Window.partitionBy(stat.web_page_id).orderBy(stat.hits.desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# .over(wind) applies the function (order by hits) for every partition\n",
    "top2 = stat.select(\"web_page_id\",\"device_id\",\"hits\", \\\n",
    "    f.dense_rank().over(wind).alias(\"r\")).where(\"r<3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Joining the tables to select the correct col names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top2_revised = top2.join(devices, top2.device_id == devices.device_num) \\\n",
    "    .join(webpage, top2.web_page_id == webpage.web_page_num) \\\n",
    "    .select(webpage.web_page_file_name, devices.device_name, top2.hits)\n",
    "    \n",
    "# Renaming \n",
    "top2_revised.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
